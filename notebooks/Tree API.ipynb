{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS IS A SKETCH, FOR DISCUSSION!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an API for dependency trees.\n",
    "\n",
    "Dependency grammars describe relationships between lexical units in a sentence.  *Unit* here means signifier: sound/meaning.  For many languages these are *words*, which I take to be mostly a phonological concept.  For agglutinative languages such as those in the Altaic and Uralic families (Turkish, Finnish), the phonological word is too large a unit: instead, we would want to work with *morphemes*.  So instead we simply use the term *form*.  Sign = Form + meaning.\n",
    "\n",
    "Dependencies are binary asymmetric relations between pairs of forms: a *governor* or *head* and a *dependent*.  What exactly are these relations?  There are many varieties of dependency grammar, taking different sets of relations as primaries of the grammar.  Nivre summarizes the possibilities, for a governor H and a depedent D:\n",
    "\n",
    "1. H determines the syntactic category of  C and can often replace C. \n",
    "2. H determines the semantic category of C; D gives semantic specification. \n",
    "3. H is obligatory; D may be optional. \n",
    "4. H selects D and determines whether D is obligatory or optional. \n",
    "5. The form ofD depends on H (agreement or government).\n",
    "6. The linear position of D is specified with reference to H.\n",
    "\n",
    "The simplest approach is to simply not state the dependency type overtly, *untyped dependencies*.  In this case the analysis  just yields a graph of binary relations between forms.\n",
    "\n",
    "Typed dependencies are triples `(head, dependent, dep-relation)`.  For any sentence, each dependent has only one governor.  A dependency graph is therefore, formally, a *tree*: a connected acyclic rooted graph.\n",
    "\n",
    "The specific sets of relations one selects in effect defines a theory of grammar.  There is much confusion around this.  For example, Stanford's CoreNLP refers to dependency graphs as \"semantic trees\", but it is immediately obvious from the relations used (see below) that these are syntactic relations. If semantics were involved, the dependency trees for `All cats are grey` and `Some cats are grey` would not be identical (they are). \n",
    "\n",
    "The [Universal Dependencies](https://universaldependencies.org/introduction.html) developed from an older set of relations developed at Stanford, provide a core set of [cross-linguistic relations](https://universaldependencies.org/u/dep/index.html) used in annotating a large number of languages.  These are the relations (and POS tags) output by the [new neural models](https://stanfordnlp.github.io/stanfordnlp/) recently released by the Stanford NLP group.\n",
    "\n",
    "Dependency linguistics has not, traditionally, been popular in the US, presumably because mainstream linguists find that dependency grammars fail to provide (even) descriptive accounts of syntactic phenomena like agreement and movement.  Computational linguists like dependencies because automatic parsers are easier to use, so they are willing to accept the limitations in exchange.  See [here](https://linguistics.stackexchange.com/questions/7280/why-is-constituency-needed-since-dependency-gets-the-job-done-more-easily-and-e) for a readable discussion. \n",
    "\n",
    "A question, then, is whether philologists might find some use for dependency parses.  To find out, we ought to provide a useful representation for working with trees.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desiderata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At work and in school I have found two main uses for automatic parsers:\n",
    "\n",
    "1.  To find specific structures in some corpus of language.  Say I'd like to find occurences of passivised subjects with past tense verbs, perhaps as a shallow feature for some statistical model.  Therefore a method for searching through parses (tree searching) is required.\n",
    "2.  Determining the syntactic parse of a whole sentence, with some sort of visualization.  This is more useful when e.g. studying the grammar of a language.\n",
    "\n",
    "I'm sure there are more.  What are they?\n",
    "\n",
    "We need a data structure to represent trees.  In its current state, the Python `stanfordnlp` project does not supply one, in contrast to the very elaborate Java [SemanticGraph](https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/SemanticGraph.html) published as part of the CoreNLP project.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A pythonic approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python provides a tree class in its core distribution: [ElementTree](https://docs.python.org/2/library/xml.etree.elementtree.html)  Of course the module is generally used for XML processing, but we may certainly commandeer it for our purposes.  It provides everything we need:\n",
    "- A data structure for nodes and their children.\n",
    "- Nodes have attribute dictionaries, for representing morphosyntactic and other features.\n",
    "- An [XPath](https://www.w3schools.com/xml/xml_xpath.asp) mechanism for searching trees.\n",
    "\n",
    "Let's see how far it gets us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by pulling a parse from the stanford toolset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/jds/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/jds/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/home/jds/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/jds/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/home/jds/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/home/jds/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "nlp = stanfordnlp.Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('In the summer of the Roman year 699, now described as the year 55 before the birth of Christ, the Proconsul of Gaul, Gaius Julius Caesar, turned his gaze upon Britain.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Document` object provides access to `Sentence` objects, and these to `Word`s and dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(stanfordnlp.pipeline.doc.Document,\n",
       " stanfordnlp.pipeline.doc.Sentence,\n",
       " stanfordnlp.pipeline.doc.Word)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc), type(doc.sentences[0]), type(doc.sentences[0].words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('In', '3', 'case')\n",
      "('the', '3', 'det')\n",
      "('summer', '31', 'obl')\n",
      "('of', '7', 'case')\n",
      "('the', '7', 'det')\n",
      "('Roman', '7', 'amod')\n",
      "('year', '3', 'nmod')\n",
      "('699', '7', 'nummod')\n",
      "(',', '31', 'punct')\n",
      "('now', '11', 'advmod')\n",
      "('described', '31', 'advcl')\n",
      "('as', '14', 'case')\n",
      "('the', '14', 'det')\n",
      "('year', '11', 'obl')\n",
      "('55', '14', 'nummod')\n",
      "('before', '18', 'case')\n",
      "('the', '18', 'det')\n",
      "('birth', '11', 'obl')\n",
      "('of', '20', 'case')\n",
      "('Christ', '18', 'nmod')\n",
      "(',', '31', 'punct')\n",
      "('the', '23', 'det')\n",
      "('Proconsul', '31', 'nsubj')\n",
      "('of', '25', 'case')\n",
      "('Gaul', '23', 'nmod')\n",
      "(',', '27', 'punct')\n",
      "('Gaius', '25', 'conj')\n",
      "('Julius', '27', 'flat')\n",
      "('Caesar', '27', 'flat')\n",
      "(',', '31', 'punct')\n",
      "('turned', '0', 'root')\n",
      "('his', '33', 'nmod:poss')\n",
      "('gaze', '31', 'obj')\n",
      "('upon', '35', 'case')\n",
      "('Britain', '31', 'obl')\n",
      "('.', '31', 'punct')\n"
     ]
    }
   ],
   "source": [
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers in each dep are indices into the word list, determining the governor in each dependency.  So \"In\" is a `case` dependent of \"summer'.  (It is quite correct to take prepositions as exponents of case relations, but this examples shows that universal dependencies are syntactic in nature)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Word` objects are richly annotated with morphosyntactic information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Word index=11;text=described;lemma=describe;upos=VERB;xpos=VBN;feats=Tense=Past|VerbForm=Part;governor=31;dependency_relation=advcl>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sentences[0].words[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this information to be visible to XPath searches it needs to be transferred to the attributes of ElementTree nodes.  A `Form` class might accomplish this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A form is fundamentally just a string.  But in a grammatical analysis we'll also want morphological information tied to the form: POS and other relevant morphosyntactic features.  So instead we'll derive from Python's `Element` class, so that we can arrange forms in `ElementTree` structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree.ElementTree import Element, ElementTree\n",
    "from xml.etree.ElementTree import dump\n",
    "from typing import List, Union\n",
    "\n",
    "\n",
    "class Form(Element):\n",
    "    def __init__(self, form : str, id :int = 0) -> None:\n",
    "        Element.__init__(self, form, attrib={'id' : str(id)})\n",
    "        \n",
    "    def __truediv__(self, pos_tag : str) -> 'Form':\n",
    "        '''\n",
    "        Sets the POS feature of this form.\n",
    "        '''\n",
    "        self.set('pos', pos_tag)\n",
    "        return self\n",
    "    \n",
    "    def __rshift__(self, other : Union['Form', str]) -> 'Dependency':\n",
    "        '''\n",
    "        Create a dependency between this form as governor, to the other as dependent.\n",
    "        Adds the dependent to the children of this form.\n",
    "        '''\n",
    "        other = Form(other) if isinstance(other, str) else other\n",
    "        self.append(other)\n",
    "        return Dependency(self, other)\n",
    "    \n",
    "    def get_dependencies(self, relation : str) -> List['Dependency']:\n",
    "        '''\n",
    "        Extract dependents of this form for the specified dependency relation.\n",
    "        '''\n",
    "        deps = self.findall('*[@relation=\"{}\"]'.format(relation))\n",
    "        return [Dependency(self, dep, relation) for dep in deps]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.tag + '_' + self('id') + (('/' + self('pos')) if self('pos') else '')\n",
    "    \n",
    "    __repr__ = __str__\n",
    "    \n",
    "    def full_str(self, include_relation=True):\n",
    "        '''\n",
    "        Returns a string containing all features of the Form.\n",
    "        The ID is attached to the text, and the relation is optionally suppressed.\n",
    "        '''\n",
    "        excluded = ['id', 'relation'] if not include_relation else ['id']\n",
    "        return '{0}_{1} [{2}]'.format(self.tag, \n",
    "                                        self('id'), \n",
    "                                        ','.join([feature + '=' + self(feature) for feature in self.attrib.keys()\n",
    "                                                 if feature not in excluded]))\n",
    "    \n",
    "    def __call__(self, feature : str) -> str:\n",
    "        return self.get(feature)\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_form(word : stanfordnlp.pipeline.doc.Word) -> 'Form':\n",
    "        '''\n",
    "        Converts a stanfordnlp Word object to a Form\n",
    "        '''\n",
    "        form = Form(word.text, id = word.index)\n",
    "        form.set('lemma', word.lemma)\n",
    "        form.set('pos', word.pos)\n",
    "        form.set('upos', word.upos)\n",
    "        form.set('xpos', word.xpos)\n",
    "\n",
    "        if word.feats != '_':\n",
    "            for f in word.feats.split('|'):\n",
    "                feature = f.split('=')\n",
    "                form.set(feature[0], feature[1])\n",
    "\n",
    "        return form\n",
    "    \n",
    "f = Form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simplest case, a `Form` is created from a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "described_0"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f('described')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each `Form` has an ID (relative to its sentence, defaulting to zero).\n",
    "\n",
    "We use `set` and `get` methods on forms to set its attributes.\n",
    "`/` is a shortcut for setting the POS feature of the form.  \n",
    "The form is callable, with a string feature, aliasing the `get` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'described_0 [Tense=Past,pos=VBN]'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_form = f('described')\n",
    "desc_form.set('Tense', 'Past')\n",
    "desc_form / 'VBN'\n",
    "desc_form.full_str() # Form.full_str() pulls in all feature speficications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More typically a word is created from a stanfordnlp `Word`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "described_11/VBN\n",
      "described_11 [lemma=describe,pos=VBN,upos=VERB,xpos=VBN,Tense=Past,VerbForm=Part]\n"
     ]
    }
   ],
   "source": [
    "desc_form = f.to_form(doc.sentences[0].words[10])\n",
    "print(desc_form)\n",
    "print(desc_form.full_str())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dependency is just a triple `(head, dep, relation)`, where the relation can be null, for an untyped dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dependency:\n",
    "    def __init__(self, head : Form, dep : Form, relation : str = None) -> None:\n",
    "        self.head = head\n",
    "        self.dep = dep\n",
    "        self.relation = relation\n",
    "        \n",
    "    def __str__(self):\n",
    "        return '{0}({1}, {2})'.format(self.relation if self.relation else '', self.head, self.dep)\n",
    "    \n",
    "    __repr__ = __str__\n",
    "    \n",
    "    def __or__(self, relation : str) -> Dependency:\n",
    "        self.relation = relation\n",
    "        self.dep.set('relation', relation)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ovreloaded `>>` operator on `Form` creates a `Dependency` and the `|` operator on `Dependency` sets the relation.  Thus we can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nsubj(wrote_0/VBN, Caesar_0/NNP)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adep = f('wrote') / 'VBN' >> f('Caesar') / 'NNP' | 'nsubj'\n",
    "adep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, these operator bear side-effects: the dependent form is made a child of the head, and the `relation` attribute is set in the dependent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Caesar_0/NNP, 'nsubj')"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adep.head[0], adep.dep.get('relation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   A `DependencyTree` is just a specialization of Python's `ElementTree`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DependencyTree(ElementTree):\n",
    "    def __init__(self, root : Form) -> None:\n",
    "        root.set('relation', 'root')\n",
    "        \n",
    "        ElementTree.__init__(self, root)\n",
    "        \n",
    "    def _get_deps(self, node : Form, deps : List[Dependency]) -> List[Dependency]:\n",
    "        for child_node in node.getchildren():\n",
    "            deps = self._get_deps(child_node, deps)\n",
    "            deps.extend(node.get_dependencies(child_node('relation')))\n",
    "        return deps\n",
    "        \n",
    "    def get_dependencies(self) -> List[Dependency]:\n",
    "        '''\n",
    "        Returns a list of all the dependency relations in the tree, generated by depth-first search.\n",
    "        '''\n",
    "        deps = self._get_deps(self.getroot(), [])\n",
    "        deps.append(Dependency(None, self.getroot(), 'root'))\n",
    "        return deps\n",
    "    \n",
    "    def _print_treelet(self, node : Form, indent : int, all_features : bool):\n",
    "        edge = '└─ ' if indent > 0 else ''\n",
    "        node_str = node.full_str(False) if all_features else str(node)\n",
    "        print(' ' * indent + edge + node('relation') + ' | ' + node_str)\n",
    "        \n",
    "        for child_node in node.getchildren():\n",
    "            self._print_treelet(child_node, indent + 4, all_features)\n",
    "    \n",
    "    def print_tree(self, all_features : bool = True):\n",
    "        '''\n",
    "        Prints a prety-printed (indented) representation of the dependency tree.\n",
    "        If all_features is True, then each node is printed with its complete feature bundle.\n",
    "        '''\n",
    "        self._print_treelet(self.getroot(), indent = 0, all_features = all_features) \n",
    "        \n",
    "    @staticmethod\n",
    "    def to_tree(sentence : stanfordnlp.pipeline.doc.Sentence) -> 'DependencyTree':\n",
    "        '''\n",
    "        Factory method to create trees from stanfordnlp sentence parses.\n",
    "        '''\n",
    "        forms = {}\n",
    "        for word in sentence.words:\n",
    "            forms[word.index] = Form.to_form(word)\n",
    "\n",
    "        for word in sentence.words:\n",
    "            if word.dependency_relation == 'root':\n",
    "                root = forms[word.index]\n",
    "            else:\n",
    "                gov = forms[str(word.governor)]\n",
    "                dep = forms[word.index]\n",
    "                gov >> dep | word.dependency_relation\n",
    "            \n",
    "        return Tree(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how this works, let's build up a tree from scratch -- presumably not a typical process for end users.\n",
    "\n",
    "Note that in dependency linguistics the root node, marked by the `root` relation and having a null governor, is typically the *head of the predicate*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root | loves_2/VRB\n",
      "    └─ nsubj | John_1/NNP\n",
      "    └─ obj | Mary_3/NNP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jds/devworld/cltk/venv/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n"
     ]
    }
   ],
   "source": [
    "john, loves, mary = f('John', 1) / 'NNP', f('loves', 2) / 'VRB', f('Mary', 3) / 'NNP'\n",
    "loves >> john | 'nsubj'\n",
    "loves >> mary | 'obj'\n",
    "t = DependencyTree(loves)\n",
    "t.print_tree(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jds/devworld/cltk/venv/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[nsubj(loves_2/VRB, John_1/NNP),\n",
       " obj(loves_2/VRB, Mary_3/NNP),\n",
       " root(None, loves_2/VRB)]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.get_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching this tree is trivial, but demonstrates the use of XPath.  The following finds all nominal subject(s) and the governor(s) of subjects: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([John_1/NNP], [loves_2/VRB])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.findall('.//*[@relation=\"nsubj\"]'), t.findall('.//*[@relation=\"nsubj\"]/..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = DependencyTree.to_tree(doc.sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root | turned_31 [lemma=turn,pos=VBD,upos=VERB,xpos=VBD,Mood=Ind,Tense=Past,VerbForm=Fin]\n",
      "    └─ obl | summer_3 [lemma=summer,pos=NN,upos=NOUN,xpos=NN,Number=Sing]\n",
      "        └─ case | In_1 [lemma=in,pos=IN,upos=ADP,xpos=IN]\n",
      "        └─ det | the_2 [lemma=the,pos=DT,upos=DET,xpos=DT,Definite=Def,PronType=Art]\n",
      "        └─ nmod | year_7 [lemma=year,pos=NN,upos=NOUN,xpos=NN,Number=Sing]\n",
      "            └─ case | of_4 [lemma=of,pos=IN,upos=ADP,xpos=IN]\n",
      "            └─ det | the_5 [lemma=the,pos=DT,upos=DET,xpos=DT,Definite=Def,PronType=Art]\n",
      "            └─ amod | Roman_6 [lemma=Roman,pos=JJ,upos=ADJ,xpos=JJ,Degree=Pos]\n",
      "            └─ nummod | 699_8 [lemma=699,pos=CD,upos=NUM,xpos=CD,NumType=Card]\n",
      "    └─ punct | ,_9 [lemma=,,pos=,,upos=PUNCT,xpos=,]\n",
      "    └─ advcl | described_11 [lemma=describe,pos=VBN,upos=VERB,xpos=VBN,Tense=Past,VerbForm=Part]\n",
      "        └─ advmod | now_10 [lemma=now,pos=RB,upos=ADV,xpos=RB]\n",
      "        └─ obl | year_14 [lemma=year,pos=NN,upos=NOUN,xpos=NN,Number=Sing]\n",
      "            └─ case | as_12 [lemma=as,pos=IN,upos=ADP,xpos=IN]\n",
      "            └─ det | the_13 [lemma=the,pos=DT,upos=DET,xpos=DT,Definite=Def,PronType=Art]\n",
      "            └─ nummod | 55_15 [lemma=55,pos=CD,upos=NUM,xpos=CD,NumType=Card]\n",
      "        └─ obl | birth_18 [lemma=birth,pos=NN,upos=NOUN,xpos=NN,Number=Sing]\n",
      "            └─ case | before_16 [lemma=before,pos=IN,upos=ADP,xpos=IN]\n",
      "            └─ det | the_17 [lemma=the,pos=DT,upos=DET,xpos=DT,Definite=Def,PronType=Art]\n",
      "            └─ nmod | Christ_20 [lemma=Christ,pos=NNP,upos=PROPN,xpos=NNP,Number=Sing]\n",
      "                └─ case | of_19 [lemma=of,pos=IN,upos=ADP,xpos=IN]\n",
      "    └─ punct | ,_21 [lemma=,,pos=,,upos=PUNCT,xpos=,]\n",
      "    └─ nsubj | Proconsul_23 [lemma=proconsul,pos=NNP,upos=PROPN,xpos=NNP,Number=Sing]\n",
      "        └─ det | the_22 [lemma=the,pos=DT,upos=DET,xpos=DT,Definite=Def,PronType=Art]\n",
      "        └─ nmod | Gaul_25 [lemma=Gaul,pos=NNP,upos=PROPN,xpos=NNP,Number=Sing]\n",
      "            └─ case | of_24 [lemma=of,pos=IN,upos=ADP,xpos=IN]\n",
      "            └─ conj | Gaius_27 [lemma=Gaius,pos=NNP,upos=PROPN,xpos=NNP,Number=Sing]\n",
      "                └─ punct | ,_26 [lemma=,,pos=,,upos=PUNCT,xpos=,]\n",
      "                └─ flat | Julius_28 [lemma=Julius,pos=NNP,upos=PROPN,xpos=NNP,Number=Sing]\n",
      "                └─ flat | Caesar_29 [lemma=Caesar,pos=NNP,upos=PROPN,xpos=NNP,Number=Sing]\n",
      "    └─ punct | ,_30 [lemma=,,pos=,,upos=PUNCT,xpos=,]\n",
      "    └─ obj | gaze_33 [lemma=gaze,pos=NN,upos=NOUN,xpos=NN,Number=Sing]\n",
      "        └─ nmod:poss | his_32 [lemma=he,pos=PRP$,upos=PRON,xpos=PRP$,Gender=Masc,Number=Sing,Person=3,Poss=Yes,PronType=Prs]\n",
      "    └─ obl | Britain_35 [lemma=Britain,pos=NNP,upos=PROPN,xpos=NNP,Number=Sing]\n",
      "        └─ case | upon_34 [lemma=upon,pos=IN,upos=ADP,xpos=IN]\n",
      "    └─ punct | ._36 [lemma=.,pos=.,upos=PUNCT,xpos=.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jds/devworld/cltk/venv/lib/python3.7/site-packages/ipykernel_launcher.py:23: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n"
     ]
    }
   ],
   "source": [
    "t1.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find all obliques in this sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[summer_3/NN, year_14/NN, birth_18/NN, Britain_35/NNP]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.findall('.//*[@relation=\"obl\"]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the children of \"Gaul\", and its parent, and all proper nouns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([of_24/IN, Gaius_27/NNP],\n",
       " Proconsul_23/NNP,\n",
       " [Christ_20/NNP,\n",
       "  Proconsul_23/NNP,\n",
       "  Gaul_25/NNP,\n",
       "  Gaius_27/NNP,\n",
       "  Julius_28/NNP,\n",
       "  Caesar_29/NNP,\n",
       "  Britain_35/NNP])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.findall('.//Gaul/*'), t1.find('.//Gaul/..'), t1.findall('.//*[@pos=\"NNP\"]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
